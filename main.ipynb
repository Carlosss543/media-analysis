{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e269ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71cf63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"./data/medias_txt\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b0eab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tous les fichiers .txt ont été vidés !\n"
     ]
    }
   ],
   "source": [
    "# effacer tout ce qu'il y a d'écrit dans les fichiers .txt du dossier output_folder\n",
    "\n",
    "for filename in os.listdir(output_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(output_folder, filename)\n",
    "        # Ouvre le fichier en mode \"w\" pour le vider\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            pass  # rien n'est écrit, donc le fichier devient vide\n",
    "\n",
    "print(\"Tous les fichiers .txt ont été vidés !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f63b0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrêt après 199 articles\n",
      "{'New Republic': 1752, 'Economist': 6574, 'Vox': 6499, 'Fox News': 13583, 'CNN': 6471, 'Axios': 4318, 'Reuters': 2848, 'Politico': 2343, 'CNBC': 4281, 'Vice': 10315, 'Vice News': 5166, 'The Verge': 5736, 'Refinery 29': 1544, 'The New York Times': 7068, 'Buzzfeed News': 5745, 'Washington Post': 5753, 'Wired': 6336, 'Mashable': 1487, 'The Hill': 2197, 'Gizmodo': 1209, 'Hyperallergic': 3688, 'TechCrunch': 2686, 'TMZ': 1521, 'People': 2036, 'New Yorker': 10445, 'Business Insider': 1602}\n"
     ]
    }
   ],
   "source": [
    "lengths = {publication: 0 for publication in ['New Republic', 'Economist', 'Vox', 'Fox News', 'CNN', 'Axios', 'Reuters', 'Politico', 'CNBC', 'Vice', 'Vice News', 'The Verge', 'Refinery 29', 'The New York Times', 'Buzzfeed News', 'Washington Post', 'Wired', 'Mashable', 'The Hill', 'Gizmodo', 'Hyperallergic', 'TechCrunch', 'TMZ', 'People', 'New Yorker', 'Business Insider']}\n",
    "nb_caracteres_max = 1000 #1_000_000\n",
    "\n",
    "for chunk in pd.read_csv('data/all-the-news-2-1.csv', chunksize=10_000):\n",
    "    for index, row in chunk.iterrows():\n",
    "    \n",
    "        media_name = row[\"publication\"]\n",
    "        article = row[\"article\"]\n",
    "\n",
    "        if type(article) is str and type(media_name) is str and lengths[media_name] < nb_caracteres_max:\n",
    "\n",
    "            article_size = len(article)\n",
    "\n",
    "            save_path = os.path.join(output_folder, f\"{media_name}.txt\")\n",
    "\n",
    "            # Écriture dans le fichier\n",
    "            with open(save_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(article + \"\\n\")\n",
    "\n",
    "            lengths[media_name] += article_size\n",
    "\n",
    "            cpt += 1\n",
    "\n",
    "    if all([length >= nb_caracteres_max for media, length in lengths.items()]):\n",
    "        print(f\"Arrêt après {cpt} articles\")\n",
    "        print(lengths)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb376a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New Republic', 'Economist', 'Vox', 'Fox News', 'CNN', 'Axios', 'Reuters', nan, 'Politico', 'CNBC', 'Vice', 'Vice News', 'The Verge', 'Refinery 29', 'The New York Times', 'Buzzfeed News', 'Washington Post', 'Wired', 'Mashable', 'The Hill', 'Gizmodo', 'Hyperallergic', 'TechCrunch', 'TMZ', 'People', 'New Yorker', 'Business Insider']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunksize = 10000\n",
    "valeurs_uniques_totales = set()\n",
    "max_iterations = 500  # nombre de chunks à lire\n",
    "\n",
    "compteur = 0\n",
    "\n",
    "for chunk in pd.read_csv('data/all-the-news-2-1.csv', chunksize=chunksize):\n",
    "\n",
    "    valeurs_uniques_totales.update(chunk['publication'].unique())\n",
    "    \n",
    "    compteur += 1\n",
    "    if compteur >= max_iterations:\n",
    "        print(f\"Arrêt après {compteur} itérations\")\n",
    "        break\n",
    "\n",
    "# Convertir en liste si nécessaire\n",
    "valeurs_uniques_liste = list(valeurs_uniques_totales)\n",
    "print(valeurs_uniques_liste)\n",
    "print(f\"Nombre total de valeurs uniques dans 'publication': {len(valeurs_uniques_totales)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9dea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv1)",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
